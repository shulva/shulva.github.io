# 机器学习

> [!NOTE] 什么是机器学习？
> _机器学习_（machine learning，ML）是一类强大的可以从经验中学习的技术。
>
> 其的核心思想是让计算机系统利用数据或经验，通过算法自动“学习”并改进其执行特定任务的能力，而无需进行显式的、针对每个具体情况的编程。

> [!NOTE] 监督学习[^1]
> 
> _监督学习_（supervised learning）擅长在“给定输入特征”的情况下预测。 
>
> 每个“特征-标签”对都称为一个_样本_（example）。 我们的目标是生成一个模型，能够将任何输入特征映射到标签（即预测）。
>
> ![](https://zh-v2.d2l.ai/_images/supervised-learning.svg)
> ###### 回归问题[^2]
> _回归_（regression）是最简单的监督学习任务之一。
>
> 当标签取任意数值时，我们称之为回归问题，此时的目标是生成一个模型，使它的预测非常接近实际标签值。
>
> 比如，预测用户对一部电影的评分可以被归类为一个回归问题。回归模型可以很好地解决**有多少(预测实际数值）**的问题
> ###### 分类问题[^3]
> _分类_问题希望模型能够预测样本属于哪个_类别_（category，正式称为_类_（class））。分类模型可以解决**是什么**的问题。
>
> **回归是训练一个回归函数来输出一个数值； 分类是训练一个分类器来输出预测的类别。**
>
> 除此之外，还存在着标记问题，搜索以及推荐系统这些ML问题[^4]

> [!question] 序列学习[^5]
> 以上大多数问题都具有固定大小的输入和产生固定大小的输出。在这些情况下，模型只会将输入作为生成输出的“原料”，而不会**记住**输入的具体内容。
>
> 如果输入的样本之间没有任何关系，以上模型可能完美无缺。 但是如果输入是连续的，模型可能就需要拥有“记忆”功能。
>
> 序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 具体来说，输入和输出都是可变长度的序列。典型的应用有自动语音识别，机器翻译以及文本转换到语音。

> [!NOTE] 无监督学习[^6]
> 如果工作没有十分具体的目标，就需要“自发”地去学习了。 比如，老板可能会给我们一大堆数据，然后要求用它做一些数据科学研究，却没有对结果有要求。 
>
> 这类**数据中不含有目标的机器学习问题**通常被为_无监督学习_（unsupervised learning）。典型的应用有聚类以及因果关系推断。

> [!question] 环境互动与强化学习[^7]
> 有人一直心存疑虑：**机器学习的输入（数据）来自哪里？机器学习的输出又将去往何方？ **
>
> 到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为_离线学习_（offline learning）。 
>
> 对于监督学习，从环境中为监督学习收集数据如图所示:
>
> ![图1.3.6](https://zh-v2.d2l.ai/_images/data-collection.svg)
> 
> 离线学习好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 这时我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。
> ###### 强化学习[^8]
>
> 如果你对使用机器学习开发与环境交互并采取行动感兴趣，那么最终可能会专注于_强化学习_（reinforcement learning）
>
>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体会:
>1：从环境接收一些_观察_（observation），并且必须选择一个_动作_（action）
>2：然后通过某种机制（有时称为执行器）将其传输回环境
>3：最后智能体从环境中获得_奖励_（reward）
>
> 此后新一轮循环开始，智能体接收后续观察，并选择后续操作。强化学习和环境之间的相互作用的过程如图所示 :
>
> ![图1.3.7](https://zh-v2.d2l.ai/_images/rl-environment.svg) 

> [!tip] 强化学习框架的通用性
> 强化学习还可以解决许多监督学习无法解决的问题。
>
> 例如，在监督学习中，我们总是希望输入与正确的标签相关联。但在强化学习中，我们并不假设环境告诉智能体每个观测的最优动作。环境甚至可能不会告诉是哪些行为导致了奖励
>
> 当环境可被完全观察到时，强化学习问题被称为_马尔可夫决策过程_（markov decision process） 
>
> 当状态不依赖于之前的操作时，我们称该问题为_上下文赌博机_（contextual bandit problem）
>
> 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的_多臂赌博机_（multi-armed bandit problem）

[^1]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id9

[^2]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id10

[^3]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id11

[^4]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id12

[^5]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id15

[^6]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id16

[^7]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id17

[^8]: https://zh-v2.d2l.ai/chapter_introduction/index.html#id18
