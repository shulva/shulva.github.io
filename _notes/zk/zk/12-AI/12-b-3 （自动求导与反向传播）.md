
# 自动求导与反向传播

深度学习框架通过自动计算导数，即**自动微分**（automatic differentiation）来加快求导。 

实际中，根据设计好的模型，系统会构建一个**计算图**（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 

![](../../../files/images/AI/12-b-3-1.png)

自动微分使系统能够随后反向传播梯度。 这里，**反向传播**（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

### 反向传播

 我们最终的目标是计算**损失函数 L** 相对于网络中**每个参数 $w_i$** 的偏导数 $∂L/∂w_i$

> [!question] 为什么在反向传播中计算梯度？
> 如果要得到一个输出$L$相对于 k 个不同的输入变量$w_i$ 的导数，通常需要进行 k 次独立的前向计算。
> 同理，如果是反向传播，那么m个输出变量就需要m次计算。
> 对于神经网络优化**这种“单输出（损失），多输入（参数）”**的场景，一次正向+一次反向计算的成本要小的多

我们仍然需要正向计算去获得中间值和最终的值来帮助反向传播计算梯度。
![](../../../files/images/AI/12-b-3-3.png)


![](../../../files/images/AI/12-b-3-4.png)

![](../../../files/images/AI/12-b-3-5.png)

![](../../../files/images/AI/12-b-3-6.png)

###  pytorch 自动求导

 假设我们想对函数$y=2\mathbf{x}^{\top}\mathbf{x}$关于列向量$\mathbf{x}$求导[^1]

 ```python
import torch

x = torch.arange(4.0)
x.requires_grad_(True) # 函数后带下划线的是原地操作的
x.grad # 默认为None
y = 2 * torch.dot(x, x)  
y.backward() # 自动计算y关于x每个分量的梯度
x.grad # tensor([ 0.,  4.,  8., 12.])

# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
y = x.sum()
y.backward()
x.grad # tensor([1., 1., 1., 1.])

 ``` 

##### 非标量变量的反向传播[^2]

当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
# y.backward(gradient=g) 实际上计算的是隐式标量L=torch.sum(y *g)
y.sum().backward() # y 中所有元素的和对其他变量的梯度
x.grad 
```

##### 分离计算[^3]

有时，我们希望将某些计算移动到记录的计算图之外。 `y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 我们想计算`z`关于`x`的梯度，但由于某种原因将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。


```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u  # tensor([True, True, True, True])

```

由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播， 得到`y=x*x`关于的`x`的导数，即`2*x`。

```python

x.grad.zero_()
y.sum().backward()
x.grad == 2 * x # tensor([True, True, True, True])

```

##### Python控制流的梯度计算[^4]
使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c # 一次函数

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()

a.grad == d / a # tensor(True)
```

[^1]: https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html#id2

[^2]: https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html#id3

[^3]: https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html#id4

[^4]: https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html#python
