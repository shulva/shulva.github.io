# 线性代数

### 向量[^1]

向量可以被视为标量值组成的列表。人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。

我们可以使用下标来引用向量的任一元素，例如通过$x_i$来引用第$i$个元素。

```python
x = torch.tensor(3.0) #标量

x = torch.arange(4)   #向量
tensor([0, 1, 2, 3])

x[3]
tensor(3)
```

##### 长度、维度和形状[^2]

向量的长度通常称为向量的_维度_（dimension）

与普通的Python数组一样，我们可以通过调用Python的内置`len()`函数来访问张量的长度。

`shape`是一个元素组，列出了张量沿每个轴的长度（维数）。

```python
len(x)
4

x.shape
torch.Size([12])

torch.tensor([[1, 2, 3], [4, 5, 6]])
torch.Size([2, 3])
```

### 矩阵[^3]

正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。

矩阵，我们通常用粗体、大写字母来表示 （eg:$\mathbf{X}$）， 在代码中表示为具有两个轴的张量。

当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为m×n的矩阵。

```python
A = torch.arange(20).reshape(5, 4)

tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
```


当我们交换矩阵的行和列时，结果称为矩阵的_转置_（transpose）,通常用$\mathbf{A}^\top$来表示矩阵的转置，其是一个是形状为n×m的矩阵。


$$
\begin{split}\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.\end{split}

\begin{split}\mathbf{A}^\top =
\begin{bmatrix}
    a_{11} & a_{21} & \dots  & a_{m1} \\
    a_{12} & a_{22} & \dots  & a_{m2} \\
    \vdots & \vdots & \ddots  & \vdots \\
    a_{1n} & a_{2n} & \dots  & a_{mn}
\end{bmatrix}.\end{split}
$$

### 张量

张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。

张量用特殊字体的大写字母表示(eg:$\mathsf{X}$)

```python
X = torch.arange(24).reshape(2, 3, 4)

tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

##### 张量算法[^4]

将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B

(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
```

两个矩阵的按元素乘法称为_Hadamard积_，$\mathbf{A}和\mathbf{B}$的的Hadamard积如下(不能与矩阵乘法混淆）：

$$
\begin{split}\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
\end{bmatrix}.\end{split}
$$

```python
A * B

tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```

### 降维[^5]

##### 降维求和
我们可以对任意张量进行的一个有用的操作是计算其元素的和,在代码中可以调用计算求和的函数：

```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
(tensor([0., 1., 2., 3.]), tensor(6.))

A.shape, A.sum()
(torch.Size([5, 4]), tensor(190.))
```

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 但我们还可以指定张量沿哪一个轴来通过求和降低维度。

以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。

```python
A_sum_axis0 = A.sum(axis=0) # 选择[5,4]中的第0个元素5
A_sum_axis0, A_sum_axis0.shape

(tensor([40., 45., 50., 55.]), torch.Size([4]))

A_sum_axis1 = A.sum(axis=1) # 选择[5,4]中的第1个元素4
A_sum_axis1, A_sum_axis1.shape
(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))

A.sum(axis=[0, 1])  # 结果和A.sum()相同
tensor(190.)
```

一个与求和相关的量是_平均值_（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。

```python
A.mean(), A.sum() / A.numel()
(tensor(9.5000), tensor(9.5000))

同样，计算平均值的函数也可以沿指定轴降低张量的维度。

A.mean(axis=0), A.sum(axis=0) / A.shape[0]
(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
```

##### 非降维求和[^6]
但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A

# 仍然是两个轴,维数变了,维度个数没有变化
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```


由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以通过[广播](12-b（pytorch基础操作）.md#广播机制[%203])将`A`除以`sum_A`。

```python
A / sum_A

tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```

如果我们想沿某个轴计算`A`元素的**累积总和**， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。

```python
A.cumsum(axis=0)

tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],  # 0+4+8 = 12
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]]) # 0+4+8+12+16 = 40
```

### 点积,向量积以及矩阵乘法

##### 点积[^7]

 给定两个向量x和y,它们的_点积_（dot product）是相同位置的按元素乘积的和：$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$

```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)

(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))

#我们也可以通过执行按元素乘法，然后进行求和来表示两个向量的点积

torch.sum(x * y)
tensor(6.)
```

##### 向量积[^8]

回顾前面章节定义的矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和向量$\mathbf{x} \in \mathbb{R}^n$,让我们将矩阵$\mathbf{A}$用它的行向量表示。

其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行

矩阵向量积$\mathbf{A}\mathbf{x}$是一个长度为$m$的列向量，其的第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$:

$$
\begin{split}\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},\end{split}

\begin{split}\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.\end{split}
$$


我们可以把一个矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$乘法看作一个从$\mathbb{R}^{n}$到$\mathbb{R}^{m}$向量的**转换**(是的，把矩阵乘法看作一个向量维度转换操作)。

在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。 

当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。(自然，乘法的要求)

```
A.shape, x.shape, torch.mv(A, x)
(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```
##### 矩阵乘法[^9]

假设有两个矩阵$\mathbf{A} \in \mathbb{R}^{n \times k}$和$\mathbf{B} \in \mathbb{R}^{k \times m}$:

$$
\begin{split}\mathbf{A}=\begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1k} \\
 a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nk} \\
\end{bmatrix},\quad
\mathbf{B}=\begin{bmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
 b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{bmatrix}.\end{split}
$$

要生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$,最简单的方法是考虑$\mathbf{A}$的行向量和$\mathbf{B}$的列向量:

$$
\begin{split}\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix},
\quad \mathbf{B}=\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}.\end{split}
$$

当我们简单地将每个元素$c_{ij}$计算为点积$\mathbf{a}^\top_i \mathbf{b}_j$

$$
\begin{split}\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.\end{split}
$$

我们可以将矩阵-矩阵乘法$\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n×m$矩阵。

```
B = torch.ones(4, 3)
torch.mm(A, B)

tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
```

> [!warning] 矩阵乘与Hadamard积
> - **标准矩阵乘法**是“行乘列求和”。主要用来表示和组合**线性变换**
> 神经网络中，权重矩阵对输入数据进行便会进行线性变换
>- **Hadamard积**是“对应元素直接乘”。对矩阵中的**每个元素独立地**进行缩放、加权或筛选
> 在循环神经网络（如LSTM、GRU）中，用Hadamard积来控制信息流的“开关”，决定哪些信息通过，哪些被遗忘
>

### 范数[^10]

简单来说，线性代数中的**范数 (Norm)** 就是一种用来**衡量向量或矩阵“大小”或“长度”**的方法。

在线性代数中，向量范数是将向量映射到标量的函数$f$。 给定任意向量$x$，向量范数要满足一些属性。

- 第一个性质是：如果我们按常数因子$α$缩放向量的所有元素， 其范数也会按相同常数因子的_绝对值_缩放

$$
f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).
$$
- 第二个性质是熟悉的三角不等式

$$
f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).
$$
- 第三个性质简单地说范数，必须是非负的

$$
f(\mathbf{x}) \geq 0.
$$


范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。

事实上，欧几里得距离是一个$L_2$范数： 假设$n$维向量$x$中的元素是$x1,…,xn$，其$L_2$范数是向量元素平方和的平方根：
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
$$

其中，在$L_2$范数中常常省略下标2，也就是说$‖\mathbf{x}‖$等同于$‖\mathbf{x}‖_2$。 在代码中，我们可以按如下方式计算向量的$L_2$范数。

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

tensor(5.)
```

深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：

$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
$$

与$L_2$范数相比，$L_1$范数受异常值的影响较小。 为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。

```python
torch.abs(u).sum()
tensor(7.)
```

> [!NOTE] Frobenius范数
>   
   $L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：
> $$
> \|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
> $$
>

[^1]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id3

[^2]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id4

[^3]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id5

[^4]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id7

[^5]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction

[^6]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#subseq-lin-alg-non-reduction

[^7]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#dot-product

[^8]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id10

[^9]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#id11

[^10]: https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms
