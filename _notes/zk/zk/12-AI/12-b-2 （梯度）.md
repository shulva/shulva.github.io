# 梯度

在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个**损失函数（loss function）**， 即一个衡量“模型有多糟糕”这个问题的分数。

最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：

- _优化_（optimization）：用模型拟合观测数据的过程；
- _泛化_（generalization）：数学原理和实践者的智慧能够指导我们生成出有效性超出用于训练的数据集本身的模型。

### 梯度[^1]

我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的_梯度_（gradient）向量。

设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是 一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，并且输出是一个标量。

则，函数$f(x)$相对于$x$的梯度是一个包含$n$个偏导数的向量:
$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top
$$

例如：$\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$

同样，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}$。

> [!NOTE] proof
>  $有f(x) =\|\mathbf{x} \|^2 = x^T x = \sum_{i=1}^{n}  x_i^2$
>
>  $有∂f/∂x_i = 2x_i$
>
> $则有[∂f/∂x_1,∂f/∂x_2...∂f/∂x_i]=2[x_1,....x_i]=2\mathbf{x}$

[直观的梯度参考](../../../files/slides/d2l/part-0_6.pdf)
![](../../../files/images/AI/12-b-2-1.png)

> [!NOTE] 分子布局
> 上图中所采用的是分子布局，即**导数的形状主要由分子（被求导的函数 y）的维度结构决定**，同理对应的有分母布局。
> 可以将其看作：标量 y 的结构是 (1,)并且列向量 x 的结构是 (n,1)。为了**匹配**分子的输出方式，∂$\mathbf{y}$/∂x 的维度在结果中表现为 (1,n)。

### 链式法则[^2]

然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是_复合_（composite）的， 所以难以应用上述任何规则来微分这些函数。 幸运的是，链式法则可以被用来微分复合函数。

让我们先考虑单变量函数。假设函数y=f(u)和u=g(x)都是可微的，根据链式法则有：
$$
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.
$$


现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。 假设可微分函数$y$有变量$u_1,u_2...u_m$，其中每个可微分函数$u_i$都有变量$x_1...x_n$。

则注意，$y$是$x_1...x_n$的函数。对于任意$i=1,2...n$，链式法则有：

$$
\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial u_1} \frac{\partial u_1}{\partial x_i} + \frac{\partial y}{\partial u_2} \frac{\partial u_2}{\partial x_i} + \cdots + \frac{\partial y}{\partial u_m} \frac{\partial u_m}{\partial x_i}
$$

![](../../../files/images/AI/12-b-2-2.png)
 

[^1]: https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html#subsec-calculus-grad

[^2]: https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html#id5

